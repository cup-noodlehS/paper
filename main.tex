\documentclass[12pt]{report}

\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage[hidelinks]{hyperref}
\usepackage{csquotes}
\usepackage[style=apa,backend=biber]{biblatex}

\addbibresource{refs.bib}

\setstretch{2}

\begin{document}

% Title page
\begin{titlepage}
  \centering
  \includegraphics[height=100pt]{images/up_logo.png}\par
  \vspace{1.5em}
  \textbf{Artificial Intelligence-Driven Traffic Violation Detection Using Multi-Object Tracking and Geometric Rules in Philippine Road Environments}\par
  \vspace{1.5em}
  \textbf{A Special Project Proposal Presented to the}\par
  \textbf{Faculty of the Department of Computer Science,}\par
  \textbf{University of the Philippines Cebu}\par
  \vspace{1.5em}
  \textbf{In Partial Fulfillment}\par
  \textbf{Of the Requirements for the Degree}\par
  \textbf{Bachelor of Science in Computer Science}\par
  \vspace{1.5em}
  \textbf{SHELDON ARTHUR M. SAGRADO}\par
  BS Computer Science\par
  \vspace{1.5em}
  \textbf{DHARRYL PRINCE ABELLANA}\par
  Special Problem Adviser\par
  \vspace{1.5em}
  \textbf{December 2025}\par
\end{titlepage}

% Approval/availability page
\begin{titlepage}
  \centering
  \includegraphics[height=100pt]{images/up_logo.png}\par
  \vspace{1.5em}
  \textbf{UNIVERSITY OF THE PHILIPPINES CEBU}\par
  \textbf{College of Science}\par
  \textbf{Department of Computer Science}\par
  \vspace{1.5em}
  \textbf{Artificial Intelligence-Driven Traffic Violation Detection Using Multi-Object Tracking and Geometric Rules in Philippine Road Environments}\par
  \vspace{1.5em}
  \textbf{Permission is given for the following people to have access to this thesis:}\par
  \vspace{1em}
  \begin{center}
    \begin{tabular}{|l|c|}
      \hline
      Available to the general public & Yes \\
      \hline
      \textbf{Available only after consultation with the author or thesis adviser} & \textbf{Yes} \\
      \hline
      \textbf{Available to those bound by a confidentiality agreement} & \textbf{Yes} \\
      \hline
    \end{tabular}
  \end{center}
  \vspace{1.5em}
  \textbf{SHELDON ARTHUR M. SAGRADO}\par
  Student\par
  \vspace{1.5em}
  \textbf{DHARRYL PRINCE ABELLANA}\par
  Special Problem Adviser\par
\end{titlepage}

\tableofcontents
\newpage

\chapter{THE PROBLEM AND ITS SCOPE}
\section{Rationale}
Road traffic injuries remain a major public-safety concern, causing about 1.19 million deaths globally each year, with the burden falling disproportionately on low- and middle-income countries where most road fatalities occur \parencite{WHO2023Road}. In the Philippines, road crashes continue to impose substantial losses, with the WHO estimating 11,062 road traffic fatalities in 2021 \parencite{WHO2023PH}. Strengthening compliance with traffic laws is therefore a practical lever for improving road safety, but fully manual enforcement is difficult to scale consistently across dense urban road networks \parencite{Olugbade2022}.

As a response, jurisdictions increasingly use camera-based enforcement and video analytics, as reflected locally by the No-Contact Apprehension Policy (NCAP) that relies on recorded visual evidence for apprehension and adjudication \parencite{MMDA2022}. For such systems, the credibility of inferred events depends on stable detection and identity continuity, because many observable violations are defined by where and how a vehicle moves within a scene \parencite{Olugbade2022,Rathore2021}.

However, real-world deployments face persistent technical issues---such as congestion, occlusion, viewpoint changes, and illumination variability---that degrade detection and tracking in traffic scenes \parencite{Wen2020}. Dataset bias and distribution shift further reduce model reliability when deployment conditions differ from training data \parencite{TorralbaEfros2011,Koh2021}. In traffic surveillance, these inconsistencies propagate into tracking instability and identity switches in multi-object tracking (MOT), undermining trajectory-based violation reasoning \parencite{Wen2020,Zhang2022}.

Accordingly, this study investigates a modular pipeline that uses MOT to support trajectory-based inference and applies interpretable geometric rules for reviewable violation decisions \parencite{Olugbade2022,Rathore2021}. The study further incorporates fuzzy logic as an explicit mechanism for handling uncertainty during association and class stabilization \parencite{Zadeh1965,Zhan2018FuzzyDA}.
\section{Research Gap}
Automated traffic violation detection (ATVD) systems commonly follow a detection--tracking--analysis pipeline to infer violations from trajectories rather than single-frame cues \parencite{Olugbade2022}. In closed-circuit television (CCTV) traffic scenes, occlusion and illumination variability can reduce detection consistency and destabilize MOT \parencite{Wen2020,Zhang2022}. Generalization gaps under distribution shift further degrade detector reliability when deployment conditions differ from training data \parencite{Koh2021,TorralbaEfros2011}. Because geometric and region-based rules operate on trajectories, identity switches and fragmented tracks undermine reliable and auditable violation inference \parencite{Olugbade2022,Rathore2021}. A remaining gap is an interpretable mechanism for handling uncertainty during association and class stabilization so that ambiguous evidence can be managed without relying solely on brittle thresholds \parencite{Zadeh1965,Zhan2018FuzzyDA}.

\section{Purpose of the Study}
In Philippine urban roads, CCTV-based enforcement such as NCAP relies on video evidence to scale monitoring and documentation of traffic violations \parencite{MMDA2022}. This study develops and evaluates an ATVD pipeline for fixed roadside CCTV video that couples real-time vehicle detection with MOT and integrates fuzzy logic to handle uncertainty during association and class stabilization \parencite{Zadeh1965,Zhan2018FuzzyDA}. The resulting trajectories are evaluated using calibrated geometric rules to infer the study's target violation events within a fixed camera view \parencite{Bell2020,RevaudHumenberger2021}. The system generates evidence bundles (annotated frames, timestamps, track identifiers, and rule-trigger traces) intended to support human review and verification in camera-based enforcement workflows \parencite{MMDA2022}.

\section{Research Questions}
ATVD for CCTV-based enforcement is commonly implemented as a detection--tracking--analysis pipeline \parencite{Olugbade2022}. Because downstream tracking and geometric inference depend on stable detections and consistent identities under occlusion and distribution shift, this study evaluates the pipeline using standard detection, tracking, and event-level metrics \parencite{BernardinStiefelhagen2008,FernandezLlorca2021}. Accordingly, this study is guided by the following research questions:

\begin{enumerate}
  \item \textbf{Detection Stability and Tracking Performance:} How does the detector operating point (e.g., confidence thresholding) affect downstream tracking continuity and identity stability in dense, occlusion-prone traffic video?
  \item \textbf{Fuzzy-Enhanced MOT Versus Baseline:} Does integrating fuzzy decision fusion for association and class stabilization improve identity-based tracking performance compared to a baseline tracking-by-detection approach?
  \item \textbf{Event-Level Violation Detection Accuracy:} Using stabilized tracks and calibrated geometric rules, how accurately can the system detect the five target violation events (including overspeed events) at the event level (precision, recall, and F1-score per violation type)?
\end{enumerate}
\section{Objectives of the Study}
This study aims to develop and evaluate an ATVD pipeline for fixed roadside CCTV video that integrates a You Only Look Once (YOLO) family detector for vehicle detection, MOT, fuzzy uncertainty handling, and geometric rule-based violation inference \parencite{Olugbade2022,RedmonFarhadi2018}. Because deployment environments can differ from common training data, the study emphasizes detector consistency and track stability under occlusion and distribution shift \parencite{Koh2021,TorralbaEfros2011}. Specifically, the study seeks to:

\begin{enumerate}
  \item Evaluate how detector operating settings affect tracking performance by measuring detection quality and downstream MOT stability across representative traffic conditions.
  \item Design and evaluate a fuzzy-enhanced MOT module for association and class stabilization, and compare it against a baseline MOT using identity-based tracking metrics.
  \item Implement calibrated geometric rules for the five target violation events and evaluate event-level detection performance, producing reviewable evidence bundles for each detected event.
\end{enumerate}

To operationalize these objectives, the study uses the following construct-to-metric mapping:

\begin{itemize}
  \item \textbf{Detection quality:} Precision, Recall, and mean Average Precision (mAP).
  \item \textbf{Tracking stability:} Multiple Object Tracking Accuracy/Precision (MOTA/MOTP) \parencite{BernardinStiefelhagen2008} and identity-based measures such as identification F1 score (IDF1) and ID switches \parencite{Ristani2016}.
  \item \textbf{Violation inference accuracy:} Event-level Precision, Recall, and F1-score computed per violation type \parencite{FernandezLlorca2021}.
\end{itemize}
\section{Theoretical Framework}
This study adopts a pipeline-oriented framework for ATVD in which a violation is treated as a temporal event inferred from object trajectories, rather than as a purely frame-level classification outcome \parencite{Olugbade2022,Wen2020}. The framework is grounded in three coupled layers: perception, temporal continuity, and rule-based violation reasoning, which reflects common ATVD system designs for traffic monitoring and enforcement \parencite{Olugbade2022,Wen2020}.

In the perception layer, YOLO-based object detection produces frame-level observations (bounding boxes and classes) for vehicles at real-time-friendly speeds, providing the primary inputs required by downstream tracking \parencite{RedmonFarhadi2018}. However, modern computer vision models trained on large datasets can exhibit dataset-specific ``signatures'' and cross-dataset generalization gaps, meaning that performance can degrade when the deployment environment differs from the training environment \parencite{Koh2021,TorralbaEfros2011}. Such distribution shifts can be amplified in traffic monitoring due to differences in camera viewpoint, local vehicle taxonomy, and scene context \parencite{Koh2021,Wen2020}.

In the temporal continuity layer, MOT is used to associate detections across frames to yield stable identities and trajectories, enabling reasoning about lane-level behaviors and movements across regions of interest \parencite{Wen2020}. Tracking-by-detection methods emphasize that identity fragmentation often arises when low-confidence detections are discarded or when association is brittle under occlusion and score fluctuations \parencite{Zhang2022}. To address uncertainty arising from imperfect detections and domain shift, this study incorporates fuzzy logic as an uncertainty-handling mechanism within MOT, leveraging the fuzzy set principle of graded membership to represent ambiguous evidence and support more stable decision-making under imprecision \parencite{Zadeh1965,Zhan2018FuzzyDA}.

In the violation reasoning layer, geometric rules translate trajectories into interpretable violation events by evaluating track movements relative to lane boundaries, stop lines, or prohibited regions, which supports auditability compared to purely end-to-end ``black-box'' offense classification \parencite{Olugbade2022,Rathore2021}. This approach aligns with prior lane- and region-based violation detection work, where road geometry (lines, lanes, and zones) acts as the explicit constraint system against which tracked movement is evaluated \parencite{Olugbade2022,Rathore2021}.

Taken together, the framework positions reliable detection as necessary but not sufficient: consistent tracking and explicit rule reasoning are treated as the core mechanisms that enable explainable, reviewable violation inference in real-world traffic surveillance \parencite{Olugbade2022,Zhang2022}.
\section{Scope and Delimitation}
This study focuses on traffic violations that can be inferred from vehicle trajectories and road geometry using monocular traffic video. It adopts a tracking-by-detection pipeline that integrates YOLO-based detection with MOT \parencite{RedmonFarhadi2018,Zhang2022}, incorporates fuzzy reasoning to manage association uncertainty \parencite{Zadeh1965,Zhan2018FuzzyDA}, and applies geometric rules for interpretable violation inference \parencite{Olugbade2022}.

The evaluated violations are limited to those observable from a single fixed camera view and operationalizable with calibrated road geometry: (1) lane misuse (restricted lane), (2) no-stopping/loading-zone violations, (3) wrong-way driving/counterflow, (4) illegal U-turns, and (5) overspeed events (speed-threshold events). When posted speed limits are available and validated for a site, the overspeed threshold will use the posted limit and the resulting events correspond to speeding violations; otherwise, results are reported as overspeed events under a research-defined threshold rather than legal speeding claims. The scope includes designing region- and lane-based rule logic (e.g., boundary crossing and prohibited region entry) that can be operationalized using user-defined lane lines, zones, and reference boundaries within the camera view \parencite{Olugbade2022,Rathore2021}. The system evaluation is constrained to scenarios where the camera perspective and scene layout are sufficiently stable to allow consistent geometric rule application, consistent with common assumptions in lane- and region-based traffic analytics \parencite{Rathore2021,Wen2020}.

The study does not aim to develop a nationwide-ready generalized detector for all Philippine roads, because distribution shift across locations, cameras, and vehicle appearances is known to degrade out-of-distribution performance for machine learning systems \parencite{Koh2021,TorralbaEfros2011}. Instead, the work emphasizes improving practical consistency in tracking and rule inference under local vehicle and scene variability by introducing fuzzy logic into MOT to manage uncertainty \parencite{Zadeh1965,Zhan2018FuzzyDA}.

The study also does not cover violations that require direct observation of rider/occupant attributes (e.g., helmet use, seatbelt use) or fine-grained interior cues, because the proposed pipeline operates at the level of vehicle detections and trajectories and does not model person-level attributes (which require different annotations and modeling assumptions). In addition, the study does not: (a) perform license plate recognition (LPR), (b) infer identities or personal attributes of road users, or (c) automate legal adjudication or issuance of citations. Rather, it focuses on generating evidence-oriented, reviewable outputs (e.g., annotated trajectories and rule-trigger traces) that can support verification within camera-based traffic monitoring or enforcement workflows \parencite{MMDA2022,Olugbade2022}.
\section{Significance of the Study}
This study is significant because road traffic injury remains a major public safety issue globally, and effective traffic law enforcement and compliance are widely recognized as important contributors to reducing crash risks and related harms \parencite{WHO2023Road}. In practice, scaling enforcement through video analytics can reduce reliance on continuous manual monitoring while enabling consistent capture of observable driving behaviors, particularly in dense urban settings where violations can be frequent and transient \parencite{Olugbade2022,Wen2020}.

For Philippine-focused deployment, the study contributes a pipeline that explicitly addresses distribution shift and its potential impact on detector consistency, tracking stability, and the reliability of rule-based violation inference \parencite{Koh2021,TorralbaEfros2011}. By integrating fuzzy logic into MOT, the study operationalizes an interpretable uncertainty-handling mechanism intended to stabilize association and class decisions under ambiguous observations \parencite{Zadeh1965,Zhan2018FuzzyDA}.

For traffic management stakeholders, the outputs of this work are structured as evidence bundles (e.g., annotated frames, trajectories, and rule-trigger traces) intended for human review rather than automated adjudication \parencite{MMDA2022,Olugbade2022}. For researchers, the study provides a reproducible methodology---including an annotation protocol, calibration checklist, and specified rule logic---to support comparative evaluation of tracking and geometric-rule violation inference in fixed-camera settings \parencite{FernandezLlorca2021}.
\section{Definition of Terms}
For clarity and consistency, the following terms are defined as they are used in this study.

\begin{description}
  \item[\textbf{ATVD}] refers to using computational methods (typically computer vision pipelines) to automatically identify traffic rule violations from visual data such as video streams by detecting road users, analyzing their movement, and inferring violation events \parencite{Olugbade2022}.
  \item[\textbf{Bounding Box}] refers to a rectangular region that localizes an object in an image, typically represented by pixel coordinates and used as the basic output unit for modern object detectors and trackers \parencite{Zhao2019}.
  \item[\textbf{Camera Calibration}] refers to estimating camera parameters (intrinsic and/or extrinsic) required to relate image measurements to scene geometry, which is commonly needed for measurement tasks such as mapping image points to real-world coordinates \parencite{HartleyZisserman2004}.
  \item[\textbf{Class Label}] refers to the categorical output assigned to a detected object (e.g., ``car,'' ``motorcycle''), produced by an object detector as part of recognition and localization \parencite{Zhao2019}.
  \item[\textbf{Confidence Score}] refers to a detector's numeric estimate of how likely a predicted bounding box contains an object (and/or belongs to a particular class), which is often used for thresholding and association in tracking-by-detection pipelines \parencite{RedmonFarhadi2018,Zhang2022}.
  \item[\textbf{Computer Vision}] refers to the field of enabling computers to extract, interpret, and reason about information from images and video, including tasks such as detection, tracking, and geometric reasoning \parencite{Szeliski2022}.
  \item[\textbf{Data Association}] refers to the process in MOT of matching detections across frames to maintain consistent object identities over time \parencite{Zhang2022}.
  \item[\textbf{Dataset Bias}] refers to systematic differences and ``signatures'' across datasets that can lead to models learning dataset-specific patterns, reducing reliability when models are applied to new environments \parencite{TorralbaEfros2011}.
  \item[\textbf{Distribution Shift}] refers to a mismatch between the training data distribution and the deployment/test distribution, which can substantially degrade model performance outside the training conditions \parencite{Koh2021}.
  \item[\textbf{Fuzzy Logic}] refers to a reasoning framework that represents uncertainty using degrees of truth (rather than binary true/false), enabling decision-making under imprecision and noisy evidence \parencite{Zadeh1965}.
  \item[\textbf{Fuzzy Set}] refers to a set in which membership is expressed in degrees (typically between 0 and 1), forming the theoretical basis for fuzzy logic and membership-based reasoning \parencite{Zadeh1965}.
  \item[\textbf{Geometric Rules}] refers to deterministic constraints defined over road geometry (e.g., lane boundaries, forbidden zones, boundary crossings) that translate tracked trajectories into interpretable violation events \parencite{Olugbade2022,Rathore2021}.
  \item[\textbf{Homography (Perspective Transformation)}] refers to a projective mapping between two planes (or between image and a planar scene representation), often used to transform image coordinates into a top-down or normalized view for geometry-based measurement \parencite{HartleyZisserman2004}.
  \item[\textbf{Identity Switch (ID Switch)}] refers to a tracking failure where a tracked identity incorrectly changes from one physical object to another, reducing trajectory reliability and downstream event inference \parencite{Zhang2022}.
  \item[\textbf{IoU}] refers to Intersection over Union, an overlap metric computed as the intersection area divided by the union area of two bounding boxes, commonly used for detection evaluation and for association logic in tracking-by-detection \parencite{RedmonFarhadi2018,Zhao2019}.
  \item[\textbf{Lane Boundary}] refers to a lane-marking line (or an operational lane delimiter in the image plane) used to define allowable vehicle movement regions for lane- and region-based violation reasoning \parencite{Rathore2021}.
  \item[\textbf{Lane Violation}] refers to a violation inferred when a vehicle's trajectory crosses or occupies a lane/region that is designated as prohibited according to lane boundaries and rule constraints defined for the scene \parencite{Olugbade2022,Rathore2021}.
  \item[\textbf{Membership Function}] refers to a function that assigns a degree of membership (e.g., 0 to 1) to an element with respect to a fuzzy set, enabling graded representation of uncertain evidence \parencite{Zadeh1965}.
  \item[\textbf{MOT}] refers to estimating object states (e.g., bounding boxes) and maintaining object identities across frames in video so that consistent trajectories can be formed over time \parencite{Zhang2022}.
  \item[\textbf{Occlusion}] refers to partial or full obstruction of an object by another object or scene element, which commonly degrades detection confidence and increases tracking difficulty in traffic scenes \parencite{Zhang2022}.
  \item[\textbf{OOD}] refers to inputs that differ meaningfully from the training distribution, often causing degraded model reliability compared with in-distribution performance \parencite{Koh2021}.
  \item[\textbf{Pre-trained Model}] refers to a model trained previously on a large dataset and later reused as a starting point for a new task or environment, often to reduce training cost and improve baseline performance \parencite{Koh2021}.
  \item[\textbf{ROI}] refers to a defined image area (e.g., lane segments, zones, stop lines) where analysis is applied, such as triggering violations when tracked trajectories enter or cross the ROI boundary \parencite{Olugbade2022}.
  \item[\textbf{Speed Estimation (Vision-Based)}] refers to estimating vehicle speed using visual measurements (e.g., displacement over time) often requiring calibration or geometric mapping from image space to real-world units \parencite{HartleyZisserman2004}.
  \item[\textbf{Tracking-by-Detection}] refers to the common MOT paradigm where an object detector runs on each frame and the tracker associates detections across frames to form identities and trajectories \parencite{Zhang2022}.
  \item[\textbf{Trajectory}] refers to the time-ordered sequence of positions (often bounding-box centers or footprints) of a tracked object across frames, used for behavior analysis and violation inference \parencite{Olugbade2022,Zhang2022}.
  \item[\textbf{YOLO}] refers to a family of single-stage object detectors that predict bounding boxes and class probabilities directly from images in a unified network, typically enabling real-time detection performance \parencite{RedmonFarhadi2018}.
\end{description}

Chapter 2 reviews prior work organized by the pipeline components operationalized in this study: ATVD workflows, vehicle detection, MOT under occlusion, uncertainty handling via fuzzy reasoning, geometric rule-based inference, and calibration-based speed estimation. Organizing the review this way clarifies how each research area directly motivates the design choices and evaluation criteria specified in Chapter 3.

\chapter{REVIEW OF RELATED LITERATURE}
\section{ATVD Pipelines for CCTV-Based Enforcement}
ATVD refers to computational systems that identify and document observable traffic infractions from sensing data, often to support monitoring and enforcement workflows \parencite{Olugbade2022}. Prior work distinguishes sensor-based approaches (e.g., radar and inductive loops) from vision-based approaches that analyze CCTV streams \parencite{Jain2021}. Vision-based ATVD is typically implemented as a modular pipeline that combines object detection, multi-object tracking, and rule or behavior analysis so that violations can be inferred from trajectories rather than single frames \parencite{Olugbade2022}.

In the Philippine context, NCAP relies on recorded visual evidence for apprehension and review, motivating outputs that are interpretable and verifiable by human evaluators \parencite{MMDA2022}.

Real-world CCTV deployments must contend with occlusion, viewpoint changes, and illumination variability that degrade camera-based detection and tracking \parencite{Wen2020}. Rain can further reduce perception reliability \parencite{Brophy2023}, and dataset bias and distribution shift can degrade model behavior when deployment conditions differ from training data \parencite{TorralbaEfros2011,Koh2021}.

Deployments that process CCTV footage must also comply with Philippine data-protection requirements, including the Data Privacy Act of 2012 and the National Privacy Commission's policy guidance on CCTV systems \parencite{RA10173,NPCCircular2024CCTV}. Implication for this study: the system is structured as a modular vision pipeline that outputs reviewable evidence bundles while scoping data handling to non-identifying analytics consistent with applicable privacy requirements.

\section{Vehicle Detection for Real-Time Traffic Video}
Vehicle detection provides per-frame bounding boxes and coarse class labels that serve as inputs to tracking and downstream rule evaluation \parencite{Zhao2019}. In real-time traffic video analytics, single-stage detectors in the YOLO family are commonly used because they predict locations and classes in one forward pass, supporting high-throughput processing of CCTV streams \parencite{RedmonFarhadi2018}. Traffic benchmarks emphasize evaluation under occlusion and varying conditions, reflecting the non-stationary nature of roadside scenes \parencite{Wen2020}.

However, dataset bias and distribution shift can cause inconsistent detections and class outputs when deployment conditions differ from training data, and these inconsistencies can propagate into tracking instability \parencite{TorralbaEfros2011,Koh2021}. Implication for this study: a YOLO-family detector is used with an explicitly evaluated operating point, and detection metrics are reported alongside tracking and event-level outcomes.

\section{Multi-Object Tracking Under Occlusion}
MOT links per-frame detections into temporally consistent trajectories by estimating object motion and solving data association across frames. Tracking-by-detection approaches are widely used in traffic analytics because they leverage strong detectors while providing stable identities that support trajectory-based behavior analysis \parencite{Bewley2016,Zhang2022}. In enforcement-oriented ATVD, these trajectories provide the temporal continuity required for event detection (e.g., boundary crossings or prohibited-zone dwell time) rather than relying on single-frame cues \parencite{Olugbade2022}.

However, traffic scenes are prone to occlusion and dense interactions that degrade detector confidence and increase identity switches, which can fragment trajectories and destabilize downstream rule evaluation \parencite{Wen2020,Zhang2022}. Recent tracking systems emphasize more robust association under imperfect detections; for example, ByteTrack associates both high- and low-confidence detections to recover objects that would otherwise be dropped, a design that aligns with CCTV conditions where targets can be small or intermittently visible \parencite{Zhang2022}. Because association remains a primary failure point under ambiguity, surveys highlight the need to combine multiple cues (motion, overlap, appearance, confidence) when matching detections to tracks \parencite{Rakai2022}.
Implication for this study: the tracking module follows a tracking-by-detection design and is evaluated using identity-based metrics to ensure stable trajectories for geometric rule inference.

\section{Uncertainty Handling via Fuzzy Decision Fusion}
Uncertainty in ATVD arises when detector scores fluctuate, class labels vary across frames, or multiple candidates yield similar association costs. Fuzzy logic provides a principled way to represent such ambiguity using graded membership rather than binary thresholds, enabling decision fusion across uncertain cues while maintaining interpretability \parencite{Zadeh1965}. Prior work demonstrates that fuzzy decision mechanisms can be integrated into data association for MOT to stabilize associations under ambiguous observations \parencite{Zhan2018FuzzyDA}. For this study, uncertainty handling is treated as a tracking-stage enhancement intended to reduce identity instability and improve the reliability of downstream geometric inference \parencite{Rakai2022}.
Implication for this study: a fuzzy inference layer is integrated into association and class stabilization to reduce identity switches under ambiguous detections.

\section{Geometric Rule-Based Violation Inference}
Rule-based violation inference evaluates vehicle trajectories against lane boundaries, stop lines, direction constraints, and prohibited regions, producing decisions that are transparent and auditable. Such geometric reasoning is commonly used in ATVD because it can encode local traffic rules explicitly and produce reviewable traces that align with evidence-oriented enforcement workflows \parencite{Olugbade2022,Rathore2021}. Compared with end-to-end offense classification, geometric rules can make the basis of each decision inspectable (e.g., which boundary was crossed and when), which is important for operational verification.
Implication for this study: violation decisions are implemented as configurable geometric and temporal rules over stabilized trajectories to preserve interpretability and reviewability.

\section{Camera Calibration and Vision-Based Speed Estimation}
Monocular CCTV speed estimation typically requires mapping image measurements to ground-plane distances through calibration, commonly via homography under an approximately planar-road assumption \parencite{HartleyZisserman2004,RevaudHumenberger2021}. Surveys emphasize that calibration quality and scene assumptions strongly influence speed accuracy and the downstream reliability of speed-related decisions \parencite{FernandezLlorca2021}. Accordingly, calibration procedures and error considerations are essential supporting components when speed-threshold (overspeed) events are among the evaluated targets \parencite{Bell2020}.
Implication for this study: calibration and speed estimation are explicitly specified and evaluated to support defensible speed-threshold (overspeed) event decisions within the defined camera view.

Taken together, the literature motivates an ATVD design that pairs real-time detection with stable tracking, explicit uncertainty handling, and interpretable geometric inference supported by calibration. Chapter 3 operationalizes these components as a modular pipeline and defines the evaluation protocol for detection, tracking, and event-level violation decisions.

\chapter{METHODOLOGY}
\section{Research Design}
This study uses a design-and-development research approach to build and evaluate an ATVD pipeline for fixed roadside CCTV video in the Philippine setting. The methodology follows a modular computer vision pipeline---vehicle detection, MOT, camera-to-road calibration, metric speed estimation, and rule-based violation inference---so that each module can be tested independently and improved without changing the overall system behavior \parencite{FernandezLlorca2021,RevaudHumenberger2021}. Because enforcement-oriented outputs require interpretable, auditable decisions, violations are inferred primarily through geometric constraints and temporal logic applied to tracked trajectories rather than end-to-end action recognition \parencite{FernandezLlorca2021}.

The study focuses on five violations observable from a single fixed camera view: (1) lane misuse (e.g., vehicle-type restricted lanes such as bus lanes), (2) no-stopping/loading-zone violations (vehicles stopping within prohibited polygons beyond a time threshold), (3) wrong-way driving/counterflowing, (4) illegal U-turns, and (5) overspeed events (speed-threshold events derived from estimated speed). The target scenario includes dense mixed traffic and frequent occlusions that can complicate detection and association \parencite{Wen2020,Zhang2022}.

\section{Study Setting and Camera Assumptions}
The target deployment scenario is a fixed roadside CCTV camera with a static viewpoint, common in traffic monitoring installations. The approach assumes: (a) the camera is stationary during recording, (b) the road segment relevant to measurement is approximately planar, and (c) violations occur within a user-defined ROI that is manually calibrated once per camera view \parencite{FernandezLlorca2021,RevaudHumenberger2021}. The planar-road assumption is widely used for monocular speed estimation pipelines because it enables mapping image points to ground-plane metric coordinates via a homography transformation \parencite{Bell2020,RevaudHumenberger2021}.
\section{Data Acquisition and Video Collection Plan}
\subsection{Data Sources}
The study will use two complementary data sources:

\begin{enumerate}
  \item \textbf{Official traffic footage (if provided):} The researcher has initiated a request to the Land Transportation Office (LTO) for sample CCTV footage. Since availability and usability are uncertain, these data are treated as optional for the initial development cycle.
  \item \textbf{Researcher-captured roadside footage:} To ensure progress independent of agency timelines, the primary plan is to record videos from footbridges or elevated sidewalks using a stable camera (e.g., tripod or fixed mount). This approach is consistent with infrastructure-style fixed-camera assumptions required for monocular calibration and speed estimation \parencite{FernandezLlorca2021}.
\end{enumerate}
\subsection{Inclusion Criteria}
Videos will be selected to satisfy: (a) visible lane markings or stable landmarks enabling manual calibration, (b) sufficient resolution to detect vehicle bounding boxes reliably, and (c) a camera view that captures vehicle motion for multiple frames \parencite{FernandezLlorca2021,RevaudHumenberger2021}.
\subsection{Data Partitioning}
To reduce leakage and overfitting to a single viewpoint, the dataset will be partitioned by location/time into development and evaluation subsets, with at least one held-out set recorded under different lighting/traffic density conditions \parencite{FernandezLlorca2021}.
\section{Ground-Truth Annotation Protocol}
To enable reproducible evaluation of detection, tracking, and event-level violation inference, the study will produce ground-truth annotations from the collected videos. Ground truth is required for computing standard MOT and event-level metrics \parencite{BernardinStiefelhagen2008,FernandezLlorca2021}.

\subsection{Annotation Targets and Label Schema}
Annotations will be produced by the researcher (primary annotator) using an annotation tool that supports video timelines, bounding boxes, and track identifiers. A second trained annotator will be involved for quality checks on a subset of the data (quality control subsection).

The annotation outputs include:

\begin{itemize}
  \item \textbf{Frame-level detection labels:} vehicle bounding boxes and coarse vehicle class labels on sampled frames (used for Precision/Recall/mAP).
  \item \textbf{Track-level labels:} consistent track identifiers linked across frames for selected video segments, with per-frame bounding boxes (used for MOT metrics such as MOTA/MOTP and IDF1).
  \item \textbf{Event-level violation labels:} start/end timestamps for each of the five violations, linked to the corresponding ground-truth track ID (used for event-level Precision/Recall/F1).
\end{itemize}

Vehicle classes will be annotated using a fixed label set consistent with the study scope: \textit{motorcycle}, \textit{car}, \textit{bus}, and \textit{truck}. Detector outputs outside these classes will be ignored for both tracking and evaluation. If the detector uses different class names for these categories, its outputs will be mapped to this four-class set for evaluation consistency.

\subsection{Event Boundary Definitions}
Violation events will be annotated with explicit temporal boundaries using the same ROI geometry (lane polygons, zones, and reference lines) defined for the camera view. Each event record includes: violation type, track ID, and start/end timestamps. Event boundaries are defined as follows:

\begin{itemize}
  \item \textbf{Lane misuse:} start when the track's ground-contact point first enters the restricted lane polygon; end when it exits the polygon.
  \item \textbf{No-stopping/loading-zone:} start when the vehicle becomes stationary inside the no-stopping/loading-zone polygon; end when it resumes motion or exits the polygon.
  \item \textbf{Wrong-way/counterflow:} start when the vehicle's motion direction becomes opposite to the lane-direction reference within the relevant ROI; end when the direction becomes compliant or the track leaves the ROI.
  \item \textbf{Illegal U-turn:} start when a vehicle initiates a U-turn maneuver inside the configured turning region (heading reversal begins); end when the maneuver completes or the vehicle exits the turning region.
  \item \textbf{Overspeed event:} start when the estimated speed exceeds the configured threshold for a sustained duration; end when it falls below the threshold for a sustained duration.
\end{itemize}

\subsection{Quality Control and Consistency Checks}
To reduce annotation noise, a subset of the labeled data will be independently annotated by a second annotator. Agreement checks will include class-label agreement and spatial overlap agreement for bounding boxes (e.g., IoU-based consistency) and temporal agreement for event boundaries within a defined tolerance. Disagreements will be resolved through adjudication, and the finalized guidelines will be used to update annotations when systematic ambiguities are identified.
\section{System Overview}
The proposed pipeline consists of the following stages:

\begin{enumerate}
  \item \textbf{Vehicle detection and classification} using a YOLO-family detector to obtain bounding boxes and class labels per frame.
  \item \textbf{Multi-object tracking} to assign stable track IDs and estimate per-vehicle trajectories over time.
  \item \textbf{Manual geometric calibration}, including (a) homography via a ground-plane reference rectangle and (b) ROIs (polygons/lines) for lanes and prohibited zones.
  \item \textbf{Metric speed estimation} by projecting a vehicle contact point to the ground plane and measuring displacement over time.
  \item \textbf{Rule-based violation inference} using geometric constraints and temporal thresholds on tracked trajectories.
  \item \textbf{Evidence packaging} (annotated frames, trajectory overlays, event timestamps, and computed measurements).
\end{enumerate}

This modular structure matches the common three-step framing of vision-based speed systems---detect, track, and convert pixel displacement to metric displacement---while keeping the violation logic transparent and adjustable for local rules \parencite{FernandezLlorca2021,RevaudHumenberger2021}.
\section{Vehicle Detection and Classification}
A YOLO-based detector will be used to localize vehicles per frame and assign coarse categories (e.g., motorcycle, car, bus, truck). Modern traffic pipelines favor deep detectors over handcrafted background subtraction because they are more robust to illumination changes and clutter \parencite{Bell2020,RevaudHumenberger2021}. Detector confidence scores will be retained to support downstream filtering and to inform association logic during tracking \parencite{RevaudHumenberger2021}.

In this study, the detector will use publicly available pre-trained weights (without fine-tuning on the collected videos) to keep the detection component simple and isolate the contribution of tracking-stage stabilization. Based on initial screening on sample clips, a YOLO26s model (Ultralytics implementation) with MS COCO pre-trained weights will serve as the fixed detection baseline \parencite{UltralyticsYOLO26,Lin2014COCO}. Earlier YOLO versions (e.g., YOLOv3) are included for historical context on one-stage detectors \parencite{RedmonFarhadi2018}. Only the four vehicle classes within scope (motorcycle, car, bus, truck) are retained; detections outside these classes are discarded during tracking and evaluation.

To mitigate detector inconsistency under occlusion and score fluctuations, the system will incorporate track-level label stabilization (fuzzy-enhanced association and class stabilization) rather than relying on per-frame class labels alone \parencite{Zhan2018FuzzyDA,Zhang2022}.

\section{Multi-Object Tracking and Fuzzy Logic Stabilization}
\subsection{Baseline Tracker}
The tracker will follow a tracking-by-detection paradigm: detections are linked frame-to-frame into trajectories using motion prediction and association costs. Contemporary MOT systems commonly combine a Kalman filter (KF) motion model with assignment algorithms and confidence handling to sustain identities under occlusion and missed detections \parencite{Bewley2016,Zhang2022}. In particular, ByteTrack's principle---leveraging both high- and low-confidence detections to recover objects that would otherwise be dropped---is aligned with CCTV conditions where detections can be noisy and small in the image \parencite{Zhang2022}.
\subsection{Motivation for Fuzzy Logic in Association}
Fixed-camera traffic scenes can include dense mixed traffic where small vehicles and close interactions increase occlusion frequency, leading to intermittent detections, identity switches, and unstable class predictions \parencite{Wen2020,Zhang2022}. Data association is a known failure point in MOT under occlusion and ambiguous observations \parencite{Rakai2022}. Fuzzy logic is suitable as an additional decision layer because it can combine multiple uncertain cues (e.g., distance, confidence, motion consistency) into a single association preference without requiring strict thresholds for every case \parencite{Zadeh1965,Rakai2022}.
\subsection{Fuzzy-Enhanced Association and Class Stabilization}
A fuzzy inference layer will be added to complement the baseline association score. For each candidate match between an existing track and a new detection, the system computes input features such as:

\begin{itemize}
  \item \textbf{IoU proximity:} overlap between predicted track box and detection box.
  \item \textbf{Motion consistency:} difference between predicted position (KF) and observed detection.
  \item \textbf{Detection confidence:} YOLO confidence for the candidate detection.
  \item \textbf{Class consistency:} agreement between the track's historical dominant class and the detection's class.
\end{itemize}

These inputs are fuzzified into linguistic variables (e.g., low/medium/high) and combined through rules such as:

\begin{itemize}
  \item IF \textit{IoU is high} AND \textit{motion error is low} AND \textit{confidence is high} THEN \textit{match strength is very high}.
  \item IF \textit{confidence is low} BUT \textit{motion error is low} AND \textit{IoU is medium/high} THEN \textit{match strength is medium} (supporting recovery under weak detections, consistent with using low-confidence detections in tracking) \parencite{Zhang2022}.
  \item IF \textit{class consistency is low} AND \textit{confidence is low} THEN \textit{match strength is low} (reducing mis-association from noisy labels).
\end{itemize}

The fuzzy association module is implemented as a Mamdani-type fuzzy inference system with triangular and/or trapezoidal membership functions for interpretability and computational simplicity \parencite{Zadeh1965}. Inputs are normalized to comparable ranges (e.g., IoU and confidence in $[0,1]$, and motion error normalized by the predicted box scale). Rule evaluation uses standard min--max composition (min for conjunction and max for aggregation), and the final fuzzy match strength is obtained using centroid defuzzification \parencite{Zadeh1965,Zhan2018FuzzyDA}. Membership parameters (e.g., breakpoints for low/medium/high) and the weighting between the fuzzy score and the baseline association cost are treated as hyperparameters and selected on a development subset to reduce identity switches while maintaining track continuity \parencite{Rakai2022,Zhang2022}.

The defuzzified output produces a fuzzy match score, which is combined with the baseline cost to decide final assignments. Separately, the track's vehicle class is stabilized by maintaining a running distribution of observed classes over a temporal window and selecting the dominant class when confidence is sufficient, rather than switching labels frame-by-frame. This design addresses association uncertainty highlighted in MOT surveys while keeping the mechanism interpretable \parencite{Rakai2022,Zadeh1965}.
\section{Manual Geometric Calibration}
\subsection{Calibration Protocol Checklist (Per Camera)}
Because monocular measurement depends on camera geometry, each camera view will be calibrated and documented using a consistent checklist. For each camera (or each distinct viewpoint), the researcher will:

\begin{enumerate}
  \item Select a clear reference frame and assign a camera-view identifier (location, date/time, and viewpoint notes).
  \item Define and save the ROIs needed for rule inference (lanes, restricted zones, stop zones, direction lines, and turning regions) (ROI definition).
  \item Identify at least one on-road reference measurement that can be physically measured in meters (e.g., a marked distance on the pavement or a rectangular region aligned with lane markings) and record the measurement procedure.
  \item Estimate the image-to-road homography using the measured reference geometry and store the resulting transformation parameters (homography calibration) \parencite{HartleyZisserman2004}.
  \item Validate calibration by projecting additional reference distances (if available) and checking that projected measurements are consistent with on-site measurements; calibration notes and any observed residual error will be reported.
  \item Fix the ground-contact point definition used for projecting detections to the road plane (ground-contact point selection) and apply the same convention consistently across all evaluations.
\end{enumerate}

If the camera viewpoint changes (e.g., the camera is moved or zoomed), ROIs and homography are recalibrated and recorded as a new camera view.
\subsection{ROI Definition}
For each camera view, the following elements will be manually annotated on a reference frame:

\begin{itemize}
  \item Lane polygons (including restricted lanes such as bus lanes).
  \item No-stopping/loading-zone polygon(s).
  \item Direction reference vectors or lane-direction lines.
  \item U-turn-related boundaries (e.g., median opening zone, prohibited turn line, or turning box).
  \item Measurement ROI where speed is considered valid.
\end{itemize}

Manual ROI calibration is necessary because traffic rules are location-specific and because a single camera view may cover multiple regulatory zones \parencite{FernandezLlorca2021}.
\subsection{Homography Calibration via Reference Rectangle}
To convert pixel motion into metric motion, the system uses a homography mapping between the image plane and the road plane under the planar-road assumption \parencite{FernandezLlorca2021,RevaudHumenberger2021}. In this study, homography will be calibrated by selecting a reference rectangle on the road plane:

\begin{enumerate}
  \item The user selects four image points corresponding to the corners of a rectangle that lies on the road surface and aligns with the scene's 3D perspective (e.g., along lane markings or a rectangular patch measurable in the real world).
  \item The real-world width and length of the rectangle are defined as fixed metric values (measured on-site when feasible, or derived from reliable map/engineering references when appropriate).
  \item These four 2D--2D correspondences define the homography matrix $H$, which maps image coordinates to ground-plane coordinates.
\end{enumerate}

This approach matches standard monocular traffic pipelines where distance conversion relies on a road-plane homography and where calibration quality strongly influences speed accuracy \parencite{Bell2020,RevaudHumenberger2021}.
\subsection{Ground-Contact Point Selection}
For each vehicle detection, a representative ground-contact point is extracted from the bounding box (commonly the midpoint of the bottom edge) and projected through the homography into ground-plane coordinates. Using the bottom midpoint is consistent with traffic surveillance speed pipelines that treat the vehicle's contact with the road as the reference point for metric displacement \parencite{Bell2020,FernandezLlorca2021}.
\section{Speed Estimation and Overspeed Event Detection}
\subsection{Instantaneous Speed Computation}
For a tracked vehicle $i$, let $(x_t, y_t)$ be the projected ground-plane coordinate at frame time $t$. The instantaneous speed is computed as:

\begin{equation}
v_t = \frac{\sqrt{(x_t - x_{t-\Delta t})^2 + (y_t - y_{t-\Delta t})^2}}{\Delta t}
\end{equation}

where $\Delta t$ is the elapsed time between frames (derived from video frame rate). This formulation is consistent with vision-based infrastructure speed estimation pipelines that compute velocity from tracked displacements after pixel-to-meter conversion \parencite{FernandezLlorca2021}.
\subsection{Temporal Smoothing}
Because monocular estimation is sensitive to jitter from detection noise and calibration error, speed values will be smoothed using a rolling window (e.g., moving average or median filter) before violation decisions are made. Smoothing is commonly used to reduce perspective-induced fluctuations and detection noise in monocular speed estimation \parencite{Bell2020,FernandezLlorca2021}.
\subsection{Overspeed Event Rule}
An overspeed event is triggered when the smoothed speed exceeds a configured speed threshold for at least $N$ consecutive frames (or for a minimum duration in seconds), adding temporal persistence to reduce false positives from single-frame spikes. When posted speed limits are available and validated for a site, the overspeed threshold will use the posted limit and the resulting events are treated as speeding violations; otherwise, evaluation focuses on overspeed events under a research-defined threshold rather than legal speeding claims \parencite{FernandezLlorca2021}.
\section{Geometric Rule-Based Violation Detection}
All violation rules operate on (a) stabilized tracks, (b) ROIs calibrated for the camera view, and (c) temporal persistence to reduce false triggers from short occlusions. Each rule outputs an event record containing track ID, violation type, start/end timestamps, and supporting measurements.

\subsection{Lane Misuse (Restricted Lane Violation)}
A lane misuse event is detected when a vehicle track's ground-contact point enters a restricted lane polygon and the stabilized vehicle class is not permitted in that lane. To avoid false detections from boundary jitter, the system uses a buffer strategy: the vehicle must remain inside the polygon for at least $T$ seconds (or $N$ frames) before a violation is confirmed, and it must exit for a minimum persistence before clearing the state. Polygon-based lane reasoning is a standard geometric approach for lane rule enforcement from surveillance video when lane boundaries are known \parencite{FernandezLlorca2021}.
\subsection{No-Stopping / Loading-Zone Violation}
For no-stopping areas, the prohibited zone is calibrated as a polygon and combined with a time threshold, as follows:

\begin{itemize}
  \item \textbf{Zone condition:} the track's ground-contact point is inside the no-stopping/loading-zone polygon.
  \item \textbf{Stop condition:} the vehicle's estimated speed is below a small threshold (near-zero) and the ground-plane displacement over a short window remains below a distance tolerance (to avoid labeling slow crawling as ``stopped'').
  \item \textbf{Duration condition:} the stop condition persists for at least $T_{\mathrm{stop}}$ seconds.
\end{itemize}

If all conditions are satisfied, the vehicle is labeled as violating no-stopping/loading-zone rules. This formulation corresponds to the general principle that infrastructure video analytics can infer stopping behavior by combining temporal persistence and motion thresholds after tracking \parencite{FernandezLlorca2021}.
\subsection{Wrong-Way Driving / Counterflow}
Wrong-way driving is inferred by comparing the vehicle's motion direction to the allowed direction of travel within the relevant lane polygon. For a short trajectory window, the direction vector is computed from consecutive projected points and compared to a lane-direction reference vector using an angle or cosine similarity threshold. If the vehicle maintains an opposite direction beyond a persistence threshold, the system triggers a wrong-way event. Direction-based reasoning is commonly used in traffic surveillance analytics where lane direction is known and tracks provide a temporal motion signal \parencite{FernandezLlorca2021}.
\subsection{Illegal U-Turn}
Illegal U-turn detection is modeled as a trajectory-pattern violation within a configured turning region:

\begin{enumerate}
  \item A turning region (polygon) and one or more boundary lines are defined (e.g., median opening area, prohibited turning line).
  \item A candidate U-turn is detected when a track exhibits a large heading reversal (e.g., direction change beyond a threshold) while being inside the turning region.
  \item The U-turn is labeled illegal if the trajectory crosses a prohibited boundary or occurs inside a ``no U-turn'' region.
\end{enumerate}

Because U-turn geometry varies by site, this rule is explicitly designed as a configurable geometric template rather than a fixed learned action classifier, keeping the enforcement logic auditable and adaptable per intersection design \parencite{FernandezLlorca2021}.
\section{Evidence Generation and System Outputs}
For each confirmed violation event, the system generates:

\begin{itemize}
  \item \textbf{Event metadata:} violation type, track ID, timestamps, zone/lane ID, and confidence indicators.
  \item \textbf{Annotated visual evidence:} frames with bounding boxes, track IDs, and violation labels.
  \item \textbf{Trajectory overlays:} projected track path relative to polygons/lines to support interpretability.
  \item \textbf{Computed measurements:} speed values for overspeed events; stop duration for no-stopping violations; direction angle for wrong-way events.
\end{itemize}

Table~\ref{tab:evidence-bundle} defines the minimum evidence bundle required per violation type to support review and verification. The bundle is designed to be non-identifying (i.e., focused on vehicle tracks and scene geometry rather than personal attributes) and consistent with the study scope.

\begin{table}[h]
  \centering
  \caption{Minimum evidence bundle per violation type.}
  \label{tab:evidence-bundle}
  \begin{tabular}{p{1.35in} p{2.55in} p{2.25in}}
    \toprule
    \textbf{Violation} & \textbf{Minimum visual evidence} & \textbf{Rule trace / measurements} \\
    \midrule
    Lane misuse (restricted lane) &
    Key frame(s) showing the vehicle inside the restricted-lane polygon with bounding box and track ID; trajectory overlay relative to the lane polygon &
    Entry/exit timestamps; stabilized vehicle class; dwell time inside polygon \\
    \midrule
    No-stopping / loading-zone &
    Frames showing (a) entry into the zone, (b) stationary period, and (c) exit; trajectory overlay relative to the zone polygon &
    Stop duration; near-zero speed/low-displacement trace during the stop interval \\
    \midrule
    Wrong-way / counterflow &
    Consecutive frames showing movement direction within the relevant lane ROI; trajectory segment overlay with lane-direction reference &
    Direction comparison value (e.g., angle or cosine similarity) over the trigger window; start/end timestamps \\
    \midrule
    Illegal U-turn &
    Frames showing the turning maneuver within the configured turning region; trajectory overlay relative to turning region and prohibited boundary lines &
    Heading reversal indicator; boundary-crossing record (if applicable); start/end timestamps \\
    \midrule
    Overspeed event &
    Frames within the measurement ROI with track ID; trajectory overlay in the ground-plane view (when available) &
    Smoothed speed trace over the event; threshold used (posted limit or experimental threshold); start/end timestamps \\
    \bottomrule
  \end{tabular}
\end{table}

Video-based enforcement systems typically require outputs that are interpretable by human reviewers; thus, the study prioritizes evidence packaging suitable for manual validation, even if final adjudication remains outside the system scope \parencite{FernandezLlorca2021}.
\section{Performance Evaluation}
\subsection{Object Detection Metrics}
Detection quality will be assessed using Precision, Recall, and mean Average Precision (mAP) on a labeled subset of frames. Detector stability matters because downstream tracking and rule inference depend on consistent localization \parencite{RevaudHumenberger2021}.
\subsection{Tracking Metrics}
Tracking performance will be evaluated using standard MOT metrics such as Multiple Object Tracking Accuracy (MOTA) and Multiple Object Tracking Precision (MOTP) \parencite{BernardinStiefelhagen2008} and identity-based measures such as IDF1 \parencite{Ristani2016}, which quantify identity switches and continuity. These metrics are widely used for comparing tracking systems under occlusion and crowded scenes \parencite{BernardinStiefelhagen2008,Rakai2022}.
\subsection{Violation Detection Metrics}
Violation detection will be evaluated at the event level using Precision, Recall, and F1-score:

\begin{itemize}
  \item \textbf{True Positive (TP):} a detected event matches a ground-truth event for the same vehicle within a defined time tolerance.
  \item \textbf{False Positive (FP):} a detected event without a matching ground-truth event.
  \item \textbf{False Negative (FN):} a ground-truth event not detected.
\end{itemize}

This event-based evaluation is appropriate because the goal is not merely frame-level classification but correct detection of temporally extended behaviors \parencite{FernandezLlorca2021}.
\section{Experimental Design}
\subsection{Sensitivity to Detector Operating Point}
To examine how detector operating settings influence downstream tracking, experiments will vary the detector confidence threshold while keeping the tracker fixed. For each threshold, the study will report detection metrics (Precision, Recall, mAP) alongside tracking metrics (e.g., IDF1 and ID switches) to characterize the trade-off between filtering low-confidence detections and maintaining track continuity under occlusion \parencite{Wen2020,Zhang2022}.
\subsection{Ablation on Fuzzy Logic Layer}
To test the contribution of fuzzy logic stabilization, experiments will compare:

\begin{itemize}
  \item \textbf{Baseline MOT:} tracking-by-detection without fuzzy association refinement.
  \item \textbf{Fuzzy-enhanced MOT:} baseline MOT + fuzzy match scoring + track-level class stabilization.
\end{itemize}

Outcomes will be compared in terms of ID switches, fragmented tracks, and downstream violation precision/recall, reflecting the dependency of geometric rules on stable trajectories \parencite{Rakai2022}.
\subsection{Robustness Across Traffic Density and Lighting}
Evaluation subsets will include (when available) daytime and nighttime scenes and both light and congested traffic. CCTV footage quality is a known limiting factor for monocular speed estimation and tracking, especially when vehicles appear small and blurry; robustness testing under these conditions is therefore necessary \parencite{RevaudHumenberger2021}.
\section{Implementation Tools and Environment}
The prototype will be implemented using Python-based computer vision libraries. Core components include:

\begin{itemize}
  \item A YOLO-family detector for per-frame vehicle detection/classification.
  \item A tracking-by-detection MOT module (e.g., KF + association; optionally ByteTrack-style confidence handling).
  \item Manual annotation utilities for polygon/line calibration.
  \item A rule engine that consumes tracks and emits violation events and evidence artifacts.
\end{itemize}

This implementation aligns with the dominant tooling ecosystem for traffic surveillance research and enables reproducibility and iterative calibration updates \parencite{FernandezLlorca2021,Zhang2022}.
\section{Ethical and Practical Considerations}
The study emphasizes traffic-rule inference from vehicle trajectories and does not require identifying drivers or passengers. Nonetheless, video collection will avoid unnecessary capture of private spaces, and data will be stored securely with access restricted to research purposes. Data handling and retention will follow the Data Privacy Act of 2012 and the National Privacy Commission's guidance on CCTV systems \parencite{RA10173,NPCCircular2024CCTV}. Any deployment claims are limited to research prototyping; enforcement-grade usage would require agency validation, calibration certification, and alignment with local legal processes for citations and evidentiary standards \parencite{FernandezLlorca2021}.

\printbibliography

\end{document}
